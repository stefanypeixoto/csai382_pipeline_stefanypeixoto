{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0f2a7097-3854-4fb2-8c7e-7528464725a8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import pandas as pd\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "df_spark = spark.table(\"labeled_step_test\")\n",
    "df = df_spark.toPandas()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d455a3fc-e6ea-4114-aa65-715005cb211b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "feature_cols_numeric = [\"distance_cm\"]\n",
    "feature_cols_categorical = [\"sensor_type\", \"device_id\"]\n",
    "label_col = \"step_label\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ba581942-716b-4b8d-922b-4192f0cca77e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X = df[feature_cols_numeric + feature_cols_categorical]\n",
    "y = df[label_col]\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cf3a1e83-1e40-4ae1-bf5d-6cc1fc2b8a95",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "numeric_transformer = StandardScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1f0fcb79-d377-42a4-bc9a-18c1d84e096f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "categorical_transformer = OneHotEncoder(handle_unknown=\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "24bc457a-a6cf-486f-9ff7-075b08317b59",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num\", numeric_transformer, feature_cols_numeric),\n",
    "        (\"cat\", categorical_transformer, feature_cols_categorical)\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "83eedc18-2e53-4fa3-97cc-0b0ef586bd1a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "pipeline = Pipeline(steps=[\n",
    "    (\"preprocess\", preprocessor)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f406786a-0ee9-463d-9258-7fe957256ad9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "pipeline.fit(X_train)\n",
    "\n",
    "X_train_transformed = pipeline.transform(X_train)\n",
    "X_test_transformed = pipeline.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "28749125-39b1-4597-b04d-04e3064c399f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import joblib\n",
    "import os\n",
    "\n",
    "base_path = \"/tmp/etl_pipeline/\"\n",
    "\n",
    "if not os.path.exists(base_path):\n",
    "    os.makedirs(base_path)\n",
    "    print(f\"Created directory: {base_path}\")\n",
    "\n",
    "joblib.dump(pipeline, os.path.join(base_path, \"stedi_feature_pipeline.pkl\"))\n",
    "joblib.dump(X_train_transformed, os.path.join(base_path, \"X_train_transformed.pkl\"))\n",
    "joblib.dump(X_test_transformed, os.path.join(base_path, \"X_test_transformed.pkl\"))\n",
    "joblib.dump(y_test, os.path.join(base_path, \"y_test.pkl\"))\n",
    "joblib.dump(y_train, os.path.join(base_path, \"y_train.pkl\"))\n",
    "\n",
    "print(\"All files saved successfully to /tmp/etl_pipeline/!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "25602293-7e49-4702-82b0-27499f474425",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##Ethics Reflection\n",
    "\n",
    "Using a consistent, reproducible feature pipeline prevents unfairness by ensuring that every data point, regardless of its source, is treated with the exact same mathematical logic. In Machine Learning, \"hidden bias\" often creeps in when we process different groups of data inconsistently, but a pipeline locks our preprocessing (like scaling and encoding) into a stable standard. This technical consistency mirrors the spiritual principle of Equity, as taught in the scriptures: God is \"no respecter of persons\" (Acts 10:34) and operates by unchanging laws. By building reliable pipelines, we ensure our models do not favor certain device types or demographics due to sloppy or varied data handling. Just as consistent spiritual habits build a stable foundation, consistent data habits build trustworthy and fair AI systems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "30d5b1b6-40cb-4df5-a35b-29e2e70a57c7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import joblib\n",
    "import joblib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.sparse import issparse\n",
    "\n",
    "base_path = \"/tmp/etl_pipeline/\"\n",
    "\n",
    "pipeline = joblib.load(base_path + \"stedi_feature_pipeline.pkl\")\n",
    "X_train_transformed = joblib.load(base_path + \"X_train_transformed.pkl\")\n",
    "X_test_transformed = joblib.load(base_path + \"X_test_transformed.pkl\")\n",
    "y_train = joblib.load(base_path + \"y_train.pkl\")\n",
    "y_test = joblib.load(base_path + \"y_test.pkl\")\n",
    "\n",
    "def to_float_matrix(arr: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Ensures that input arrays (possibly object-dtype, sparse, or 0-d) are converted to a 2-D float matrix.\n",
    "    This is necessary because saved feature arrays may have inconsistent shapes or types after transformation,\n",
    "    and ML models require numeric 2-D arrays for training and prediction.\n",
    "    \"\"\"\n",
    "    if arr.ndim == 0:\n",
    "        arr = arr.item()\n",
    "        if issparse(arr):\n",
    "            arr = arr.toarray()\n",
    "        arr = np.array(arr, dtype=float)\n",
    "    elif arr.dtype == object:\n",
    "        arr = np.array([\n",
    "            x.toarray() if issparse(x) else np.array(x, dtype=float)\n",
    "            for x in arr\n",
    "        ])\n",
    "        arr = np.vstack(arr)\n",
    "    elif issparse(arr):\n",
    "        arr = arr.toarray()\n",
    "    else:\n",
    "        arr = np.array(arr, dtype=float)\n",
    "    return arr\n",
    "\n",
    "X_train = to_float_matrix(X_train_transformed)\n",
    "X_test = to_float_matrix(X_test_transformed)\n",
    "y_train = np.ravel(y_train)\n",
    "y_test = np.ravel(y_test)\n",
    "X_train.shape, X_test.shape, y_train.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2076be00-c508-4702-a05b-e3542b5257a1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "log_reg = LogisticRegression(max_iter=300)\n",
    "log_reg.fit(X_train, y_train)\n",
    "\n",
    "log_reg_score = log_reg.score(X_test, y_test)\n",
    "log_reg_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "882349a1-36e4-405d-985e-77e11ca99671",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rf = RandomForestClassifier()\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "rf_score = rf.score(X_test, y_test)\n",
    "rf_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ea0817da-cf60-4d6f-ae25-5acc280ddc84",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "results = {\n",
    "    \"Logistic Regression baseline\": log_reg_score,\n",
    "    \"Random Forest baseline\": rf_score\n",
    "}\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "18597bba-50e0-47b7-93d0-1ce14dbb4c82",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##Baseline Model Analysis\n",
    "\n",
    "In this baseline evaluation, the Logistic Regression model performed slightly better with an accuracy of 95.11%, compared to 95.09% for the Random Forest. While Logistic Regression was marginally more accurate here, Random Forest often proves more stable for noisy sensor data because its ensemble nature (using multiple decision trees) is less likely to be \"tricked\" by individual outliers or sensor glitches.\n",
    "\n",
    "The fact that the numbers are so close, and so high, leads me to wonder if the distance_cm feature provides a very clear linear signal for a \"step,\" or if the dataset is well-balanced. It is important to test these models before deployment because an untested model could provide false health metrics; a wrong prediction could affect patients relying on accurate step counts for rehabilitation or elderly monitoring. Therefore, fairness matters in data science just as it does in discipleship because we have a responsibility to ensure our tools serve everyone equitably. Just as we are called to treat all people with integrity, our models must not harbor \"hidden\" biases that disadvantage certain users based on their device data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8d175f34-2215-4c1f-87d8-5630175e3f91",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import joblib\n",
    "from datetime import datetime\n",
    "\n",
    "# Create a unique folder name (prevents overwriting files)\n",
    "run_id = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "base_dir = f\"/Workspace/Users/stef4@ensign.edu/stedi_models/{run_id}\"\n",
    "os.makedirs(base_dir, exist_ok=True)\n",
    "\n",
    "# Save trained models\n",
    "joblib.dump(log_reg, f\"{base_dir}/log_reg.joblib\")\n",
    "joblib.dump(rf, f\"{base_dir}/random_forest.joblib\")\n",
    "\n",
    "# Save accuracy information (metadata)\n",
    "metadata = {\n",
    "    \"run_id\": run_id,\n",
    "    \"logistic_regression_accuracy\": float(log_reg_score),\n",
    "    \"random_forest_accuracy\": float(rf_score),\n",
    "}\n",
    "\n",
    "joblib.dump(metadata, f\"{base_dir}/metadata.joblib\")\n",
    "\n",
    "base_dir\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "49fcafb3-f567-4d8e-af8a-a398f4985bc7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import shutil\n",
    "zip_path = f\"/Workspace/Users/stef4@ensign.edu/stedi_models/{run_id}.zip\"\n",
    "shutil.make_archive(zip_path.replace(\".zip\", \"\"), \"zip\", base_dir)\n",
    "zip_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3f42dc81-3dcc-4aab-ba13-528664247b8c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##5.3 Trained ML Models: Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "79007e9f-d14e-45ec-ba8f-ed46992f9df9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "log_reg_params = {\n",
    "    \"C\": [0.01, 0.1, 1, 10],\n",
    "    \"penalty\": [\"l2\"],\n",
    "    \"solver\": [\"lbfgs\", \"liblinear\"]\n",
    "}\n",
    "\n",
    "log_reg_grid = GridSearchCV(\n",
    "    LogisticRegression(max_iter=300),\n",
    "    log_reg_params,\n",
    "    cv=3,\n",
    "    scoring=\"accuracy\"\n",
    ")\n",
    "\n",
    "log_reg_grid.fit(X_train, y_train)\n",
    "\n",
    "log_reg_best_params = log_reg_grid.best_params_\n",
    "log_reg_best_score = log_reg_grid.best_score_\n",
    "\n",
    "log_reg_best_params, log_reg_best_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b7368e2d-5f97-4a3b-8304-1dd6566e0981",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rf_params = {\n",
    "    \"n_estimators\": [50, 100, 200],\n",
    "    \"max_depth\": [None, 5, 10, 20],\n",
    "    \"min_samples_split\": [2, 5],\n",
    "    \"min_samples_leaf\": [1, 2]\n",
    "}\n",
    "\n",
    "rf_grid = GridSearchCV(\n",
    "    RandomForestClassifier(),\n",
    "    rf_params,\n",
    "    cv=3,\n",
    "    scoring=\"accuracy\",\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "rf_grid.fit(X_train, y_train)\n",
    "\n",
    "rf_best_params = rf_grid.best_params_\n",
    "rf_best_score = rf_grid.best_score_\n",
    "\n",
    "rf_best_params, rf_best_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "26a918c4-3fae-4b7f-8817-9302c72828b6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "results = {\n",
    "    \"Logistic Regression (tuned)\": log_reg_best_score,\n",
    "    \"Random Forest (tuned)\": rf_best_score\n",
    "}\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d426ef47-802d-4f86-939a-825ee37f5230",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Choose the better model based on best_score_\n",
    "if rf_best_score > log_reg_best_score:\n",
    "    best_model = rf_grid.best_estimator_\n",
    "    best_model_name = \"Random Forest\"\n",
    "else:\n",
    "    best_model = log_reg_grid.best_estimator_\n",
    "    best_model_name = \"Logistic Regression\"\n",
    "\n",
    "best_model_name, best_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "70619c49-7f9c-439a-b1c0-55a995792894",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "save_path = \"/Workspace/Users/stef4@ensign.edu/stedi_models/stedi_best_model.pkl\"\n",
    "joblib.dump(best_model, save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "953ab3f7-1b66-4a49-bdbc-06e596938bd6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(f\"Successfully saved the {best_model_name} model to {save_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a9b4a72e-d4f0-4416-8961-cec64ef357ca",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##5.3 Model Evaluation Report\n",
    "\n",
    "After running hyperparameter tuning, both the Logistic Regression and Random Forest models achieved an identical cross-validation accuracy of 95.11%. This suggests that the signal in the STEDI sensor data is strong enough that even a linear model can capture it as effectively as a complex ensemble of trees. I chose to proceed with the [Random Forest / Logistic Regression] as my final model.\n",
    "\n",
    "##Ethics Reflection\n",
    "\n",
    "Hyperparameter tuning can accidentally introduce bias if we optimize solely for a single global metric like accuracy. For example, a specific setting might increase the overall score by better predicting the majority group while significantly decreasing performance for a smaller demographic. This creates a \"hidden\" unfairness that documentation and transparency help reveal. Transparency is essential because it allows others to audit our choices and ensure the model serves everyone equitably. The gospel principle of Honest Evaluation reminds us that \"by small and simple things are great things brought to pass\" (Alma 37:6); being truthful about our model’s limitations is just as important as reporting its successes. We have a responsibility to seek light and truth in our data, ensuring our technical work reflects integrity and accountability.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a20516e9-372f-482f-8033-96d9648e1b5a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Ensure model is defined as your tuned winner\n",
    "model = best_model \n",
    "\n",
    "try:\n",
    "    feature_names = pipeline.named_steps[\"preprocess\"].get_feature_names_out()\n",
    "    print(\"Ready! Feature names loaded.\")\n",
    "except:\n",
    "    print(\"Note: Using generic feature names.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6206af1e-ad20-41d6-8c5a-fa89405d4054",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "importances = model.feature_importances_\n",
    "importance_order = np.argsort(importances)[::-1]\n",
    "\n",
    "# Get feature names if available\n",
    "try:\n",
    "    feature_names = pipeline.named_steps[\"preprocess\"].get_feature_names_out()\n",
    "except:\n",
    "    feature_names = [f\"feature_{i}\" for i in range(X_train.shape[1])]\n",
    "\n",
    "for idx in importance_order[:10]:\n",
    "    print(feature_names[idx], \":\", importances[idx])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "56b72bcf-8f00-4400-bf1f-ae8cfcd268a6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "###Analysis: Feature Importance\n",
    "\n",
    "Do the most important features make sense?\n",
    "\n",
    "Yes, it makes sense that distance_cm is a primary indicator. If the STEDI device is measuring the distance from a sensor to a person's leg or the floor, that distance will change drastically and consistently every time a step is taken. The fact that different sensor types (gyroscope vs. accelerometer) also appear shows the model is using motion data, but they are dwarfed by the distance metric.\n",
    "\n",
    "Are there any surprises?\n",
    "\n",
    "The biggest surprise is how much the model relies on distance_cm. Having one feature account for over 92% of the importance is unusual. It suggests the model has found a \"shortcut.\" Also, it’s interesting that specific device_id values (like spotter-14) show up in the top 10. Ideally, a model should predict steps based on how a person moves, not which specific device they are using.\n",
    "\n",
    "Would you trust predictions made with this importance pattern?\n",
    "\n",
    "I would trust them, but with caution. Because the model is so dependent on one feature, if that distance sensor gets dusty, blocked, or glitches, the entire model's accuracy will likely collapse. For a high-stakes informatics application (like at UCLA or UCI), we usually prefer a more balanced model that uses multiple sensors (accelerometer + gyro + distance) so that there is \"redundancy\" if one sensor fails."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cfdbccb4-9fd0-4ae1-9066-ca1ffcd62d28",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.barh([feature_names[i] for i in importance_order[:10]],\n",
    "         importances[importance_order[:10]])\n",
    "plt.xlabel(\"Importance\")\n",
    "plt.title(\"Top Global Feature Importance\")\n",
    "plt.gca().invert_yaxis()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8bcba22d-6484-472f-bd91-eb4c6bcf3f81",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install shap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2552ac66-b9fc-42c4-8d91-e9c3c46457b7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import shap\n",
    "shap.initjs()\n",
    "\n",
    "explainer = shap.TreeExplainer(model)\n",
    "shap_values = explainer.shap_values(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7aeee840-087d-4585-9367-febb83f6993d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Verify the output\n",
    "print(f\"SHAP values calculated. Type: {type(shap_values)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a7c8dc13-0255-4b2a-8629-f1ac64f3c51f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "shap.summary_plot(shap_values[...,1], X_test, feature_names=feature_names, rng=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f9c3686a-bf4c-4df7-a0e5-b1b212006771",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "###SHAP Summary Plot Observations\n",
    "Match with Global Importance: The SHAP plot confirms the global importance chart; num__distance_cm is overwhelmingly the most influential feature, as it shows the widest spread of SHAP values.\n",
    "\n",
    "Direction of Influence: * For num__distance_cm, blue dots (low distance values) are clustered on the right side of the center line, meaning lower distances push the model toward predicting a step.\n",
    "\n",
    "Red dots (high distance values) are mostly on the left, pushing the prediction toward no_step.\n",
    "\n",
    "Unexpected Influences: It is surprising that the specific device_id (like spotter-14 and spotter-26) has a visible impact. In a robust model, the physical movement (accelerometer/gyro) should matter more than which specific hardware is being used, suggesting the model might be slightly overfitted to specific devices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ea3eb233-2de6-42ba-92ac-4522e10e98d7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "i = 0  # choose any index you like\n",
    "\n",
    "shap.force_plot(explainer.expected_value[1],\n",
    "                shap_values[...,1][i],\n",
    "                X_test[i],\n",
    "                feature_names=feature_names,\n",
    "                matplotlib=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "69e266b7-d28f-4a09-a43c-67fd6a34a85b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "###SHAP Force Plot Interpretation\n",
    "Do you understand the explanation?\n",
    "Yes. The force plot shows the \"tug-of-war\" between features that push the prediction away from the base value. The red arrows (like cat__sensor_type_accelerometer) are pushing the probability \"higher\" toward a step, while the blue arrows (like num__distance_cm and a specific device_id) are pulling it \"lower.\" The final result is the bold value of 0.94.\n",
    "\n",
    "Do features that push toward “step” match your expectations?\n",
    "Mostly, yes. Seeing the accelerometer as a positive (red) force makes sense, as physical movement is the primary indicator of a step. However, it is interesting that for this specific instance, the distance_cm value is actually acting as a negative (blue) force, meaning this particular distance reading made the model less certain it was a step compared to the average.\n",
    "\n",
    "Would you reach the same conclusion if you looked at the data yourself?\n",
    "If I saw a high reading from the accelerometer (value = 1.0) alongside a distance reading of ~0.58, I would likely agree that some form of movement is occurring. However, because the distance feature is so dominant globally, it’s harder for a human to weigh these small decimal differences as precisely as the model does. The visualization helps bridge that gap by showing exactly how the model balances the sensor types against the distance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7eb35b4e-08ed-4182-9c40-38bcc02cb199",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "###Final Reflection Questions\n",
    "Since you are at the end, here is a concise block to answer the final reflection section:\n",
    "\n",
    "Global Insight: The num__distance_cm feature is the most important overall, likely because the proximity of the user's leg to the sensor provides a very clean signal for walking patterns.\n",
    "\n",
    "Local Insight: The SHAP force plot revealed that while the accelerometer pushed the prediction up, the distance and device ID pulled it slightly down for this specific row, resulting in a 94% probability.\n",
    "\n",
    "Human Intuition Check: The logic mostly matches; movement (accelerometer) should predict a step. However, the heavy reliance on a single distance feature might be a \"shortcut\" that a human might be more skeptical of in varied environments.\n",
    "\n",
    "Dashboard Preparation: I plan to include the Global Feature Importance bar chart and the SHAP Summary Plot. These provide a clear \"personality portrait\" of the model for the Week 7 dashboard."
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "4.3_Feature_Engineering_Pipeline",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
