{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0f2a7097-3854-4fb2-8c7e-7528464725a8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import pandas as pd\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "df_spark = spark.table(\"labeled_step_test\")\n",
    "df = df_spark.toPandas()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d455a3fc-e6ea-4114-aa65-715005cb211b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "feature_cols_numeric = [\"distance_cm\"]\n",
    "feature_cols_categorical = [\"sensor_type\", \"device_id\"]\n",
    "label_col = \"step_label\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ba581942-716b-4b8d-922b-4192f0cca77e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X = df[feature_cols_numeric + feature_cols_categorical]\n",
    "y = df[label_col]\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cf3a1e83-1e40-4ae1-bf5d-6cc1fc2b8a95",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "numeric_transformer = StandardScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1f0fcb79-d377-42a4-bc9a-18c1d84e096f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "categorical_transformer = OneHotEncoder(handle_unknown=\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "24bc457a-a6cf-486f-9ff7-075b08317b59",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num\", numeric_transformer, feature_cols_numeric),\n",
    "        (\"cat\", categorical_transformer, feature_cols_categorical)\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "83eedc18-2e53-4fa3-97cc-0b0ef586bd1a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "pipeline = Pipeline(steps=[\n",
    "    (\"preprocess\", preprocessor)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f406786a-0ee9-463d-9258-7fe957256ad9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "pipeline.fit(X_train)\n",
    "\n",
    "X_train_transformed = pipeline.transform(X_train)\n",
    "X_test_transformed = pipeline.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "28749125-39b1-4597-b04d-04e3064c399f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import joblib\n",
    "import os\n",
    "\n",
    "base_path = \"/tmp/etl_pipeline/\"\n",
    "\n",
    "if not os.path.exists(base_path):\n",
    "    os.makedirs(base_path)\n",
    "    print(f\"Created directory: {base_path}\")\n",
    "\n",
    "joblib.dump(pipeline, os.path.join(base_path, \"stedi_feature_pipeline.pkl\"))\n",
    "joblib.dump(X_train_transformed, os.path.join(base_path, \"X_train_transformed.pkl\"))\n",
    "joblib.dump(X_test_transformed, os.path.join(base_path, \"X_test_transformed.pkl\"))\n",
    "joblib.dump(y_test, os.path.join(base_path, \"y_test.pkl\"))\n",
    "joblib.dump(y_train, os.path.join(base_path, \"y_train.pkl\"))\n",
    "\n",
    "print(\"All files saved successfully to /tmp/etl_pipeline/!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "25602293-7e49-4702-82b0-27499f474425",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##Ethics Reflection\n",
    "\n",
    "Using a consistent, reproducible feature pipeline prevents unfairness by ensuring that every data point, regardless of its source, is treated with the exact same mathematical logic. In Machine Learning, \"hidden bias\" often creeps in when we process different groups of data inconsistently, but a pipeline locks our preprocessing (like scaling and encoding) into a stable standard. This technical consistency mirrors the spiritual principle of Equity, as taught in the scriptures: God is \"no respecter of persons\" (Acts 10:34) and operates by unchanging laws. By building reliable pipelines, we ensure our models do not favor certain device types or demographics due to sloppy or varied data handling. Just as consistent spiritual habits build a stable foundation, consistent data habits build trustworthy and fair AI systems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "30d5b1b6-40cb-4df5-a35b-29e2e70a57c7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import joblib\n",
    "import joblib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.sparse import issparse\n",
    "\n",
    "base_path = \"/tmp/etl_pipeline/\"\n",
    "\n",
    "pipeline = joblib.load(base_path + \"stedi_feature_pipeline.pkl\")\n",
    "X_train_transformed = joblib.load(base_path + \"X_train_transformed.pkl\")\n",
    "X_test_transformed = joblib.load(base_path + \"X_test_transformed.pkl\")\n",
    "y_train = joblib.load(base_path + \"y_train.pkl\")\n",
    "y_test = joblib.load(base_path + \"y_test.pkl\")\n",
    "\n",
    "def to_float_matrix(arr: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Ensures that input arrays (possibly object-dtype, sparse, or 0-d) are converted to a 2-D float matrix.\n",
    "    This is necessary because saved feature arrays may have inconsistent shapes or types after transformation,\n",
    "    and ML models require numeric 2-D arrays for training and prediction.\n",
    "    \"\"\"\n",
    "    if arr.ndim == 0:\n",
    "        arr = arr.item()\n",
    "        if issparse(arr):\n",
    "            arr = arr.toarray()\n",
    "        arr = np.array(arr, dtype=float)\n",
    "    elif arr.dtype == object:\n",
    "        arr = np.array([\n",
    "            x.toarray() if issparse(x) else np.array(x, dtype=float)\n",
    "            for x in arr\n",
    "        ])\n",
    "        arr = np.vstack(arr)\n",
    "    elif issparse(arr):\n",
    "        arr = arr.toarray()\n",
    "    else:\n",
    "        arr = np.array(arr, dtype=float)\n",
    "    return arr\n",
    "\n",
    "X_train = to_float_matrix(X_train_transformed)\n",
    "X_test = to_float_matrix(X_test_transformed)\n",
    "y_train = np.ravel(y_train)\n",
    "y_test = np.ravel(y_test)\n",
    "X_train.shape, X_test.shape, y_train.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2076be00-c508-4702-a05b-e3542b5257a1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "log_reg = LogisticRegression(max_iter=300)\n",
    "log_reg.fit(X_train, y_train)\n",
    "\n",
    "log_reg_score = log_reg.score(X_test, y_test)\n",
    "log_reg_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "882349a1-36e4-405d-985e-77e11ca99671",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rf = RandomForestClassifier()\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "rf_score = rf.score(X_test, y_test)\n",
    "rf_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ea0817da-cf60-4d6f-ae25-5acc280ddc84",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "results = {\n",
    "    \"Logistic Regression baseline\": log_reg_score,\n",
    "    \"Random Forest baseline\": rf_score\n",
    "}\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "18597bba-50e0-47b7-93d0-1ce14dbb4c82",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##Baseline Model Analysis\n",
    "\n",
    "In this baseline evaluation, the Logistic Regression model performed slightly better with an accuracy of 95.11%, compared to 95.09% for the Random Forest. While Logistic Regression was marginally more accurate here, Random Forest often proves more stable for noisy sensor data because its ensemble nature (using multiple decision trees) is less likely to be \"tricked\" by individual outliers or sensor glitches.\n",
    "\n",
    "The fact that the numbers are so close, and so high, leads me to wonder if the distance_cm feature provides a very clear linear signal for a \"step,\" or if the dataset is well-balanced. It is important to test these models before deployment because an untested model could provide false health metrics; a wrong prediction could affect patients relying on accurate step counts for rehabilitation or elderly monitoring. Therefore, fairness matters in data science just as it does in discipleship because we have a responsibility to ensure our tools serve everyone equitably. Just as we are called to treat all people with integrity, our models must not harbor \"hidden\" biases that disadvantage certain users based on their device data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8d175f34-2215-4c1f-87d8-5630175e3f91",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import joblib\n",
    "from datetime import datetime\n",
    "\n",
    "# Create a unique folder name (prevents overwriting files)\n",
    "run_id = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "base_dir = f\"/Workspace/Users/stef4@ensign.edu/stedi_models/{run_id}\"\n",
    "os.makedirs(base_dir, exist_ok=True)\n",
    "\n",
    "# Save trained models\n",
    "joblib.dump(log_reg, f\"{base_dir}/log_reg.joblib\")\n",
    "joblib.dump(rf, f\"{base_dir}/random_forest.joblib\")\n",
    "\n",
    "# Save accuracy information (metadata)\n",
    "metadata = {\n",
    "    \"run_id\": run_id,\n",
    "    \"logistic_regression_accuracy\": float(log_reg_score),\n",
    "    \"random_forest_accuracy\": float(rf_score),\n",
    "}\n",
    "\n",
    "joblib.dump(metadata, f\"{base_dir}/metadata.joblib\")\n",
    "\n",
    "base_dir\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "49fcafb3-f567-4d8e-af8a-a398f4985bc7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import shutil\n",
    "zip_path = f\"/Workspace/Users/stef4@ensign.edu/stedi_models/{run_id}.zip\"\n",
    "shutil.make_archive(zip_path.replace(\".zip\", \"\"), \"zip\", base_dir)\n",
    "zip_path"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "4.3_Feature_Engineering_Pipeline",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
