{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6c563a90-377b-48ba-a663-d9057892004b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Part 2: Python Data Transformation\n",
    "In this section, I am using PySpark to convert the raw numeric timestamps into human-readable Date/Time formats for better analysis.\n",
    "\n",
    "This Python section mirrors the SQL steps, but prepares a compact features table (avg/min/max/variance of distance within each test window). We will reuse this features table in ML weeks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "46d60c25-5083-4606-97d6-3637b91aa77e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 2.1 Load tables and Inspect Schemas\n",
    "from pyspark.sql.functions import from_unixtime, col\n",
    "\n",
    "# Load the dataframes using your specific table names [cite: 38]\n",
    "dm = spark.read.table(\"workspace.bronze.device_message_raw\")\n",
    "rt = spark.read.table(\"workspace.bronze.Rapid_step_test_raw\")\n",
    "\n",
    "# Lab Requirement: Print the schemas to verify column types \n",
    "dm.printSchema()\n",
    "rt.printSchema()\n",
    "\n",
    "# Your existing transformation for readable time\n",
    "df_refined = dm.withColumn(\"readable_time\", from_unixtime(col(\"timestamp\") / 1000))\n",
    "display(df_refined.select(\"device_id\", \"timestamp\", \"readable_time\").limit(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "55caf2dc-e09c-4947-805e-7726b2e15ed2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "###Per-device descriptive stats\n",
    "\n",
    "Showing summarized stats for each device."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5f1f57a7-5c34-40a4-b7e5-28ddadaaf778",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "dm_clean = (dm\n",
    "    .withColumn(\"distance_str\", F.col(\"distance\").cast(\"string\"))\n",
    "    .withColumn(\"distance_cm\", F.regexp_replace(\"distance_str\", \"[^0-9.]\", \"\").cast(\"double\"))\n",
    "    .withColumn(\"ts_ms\", F.col(\"timestamp\").cast(\"bigint\")))\n",
    "\n",
    "dm_stats = (dm_clean\n",
    "    .groupBy(\"device_id\")\n",
    "    .agg(F.count(\"*\").alias(\"n\"),\n",
    "         F.avg(\"distance_cm\").alias(\"avg_cm\"),\n",
    "         F.min(\"distance_cm\").alias(\"min_cm\"),\n",
    "         F.max(\"distance_cm\").alias(\"max_cm\"))\n",
    "    .orderBy(F.desc(\"n\")))\n",
    "\n",
    "display(dm_stats.limit(20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ed1c1d63-6664-4da7-9b68-5f82660d7305",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "###Explode stepPoints to long form\n",
    "\n",
    "This is the python version of the explode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f46b13ff-ea1c-440c-8920-cc0b88291c1f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "rt = spark.table(\"workspace.bronze.rapid_step_test_raw\")\n",
    "rt_exploded = rt.select(\"customer\", \"device_id\", \"start_time\", \"stop_time\", \n",
    "                        F.posexplode(\"step_points\").alias(\"step_index\", \"step_ms\"))\n",
    "display(rt_exploded.limit(20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "02a025c7-b1bb-4979-97b2-18dbc4b93967",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "###Windowed join: feature prep\n",
    "\n",
    "This Python section mirrors the SQL steps, but prepares a compact features table. We will reuse this features table in ML weeks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d42488d9-095c-4fcc-92ed-328e82fddb84",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dm_clean = spark.table(\"workspace.bronze.device_message_raw\").withColumn(\"ts_ms\", F.col(\"timestamp\").cast(\"bigint\"))\n",
    "msgs = dm_clean.select(\"device_id\", \"ts_ms\", \"distance\") # Filtered distance logic [cite: 48]\n",
    "joined = rt.alias(\"t\").join(msgs.alias(\"m\"), \n",
    "         (F.col(\"m.device_id\") == F.col(\"t.device_id\")) & \n",
    "         (F.col(\"m.ts_ms\").between(F.col(\"t.start_time\"), F.col(\"t.stop_time\"))), \"inner\")\n",
    "display(joined.limit(20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c9e023d7-0314-4b24-b36d-99c14a5ecb6d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "###Quick Visual Check\n",
    "\n",
    "Creating a simple line plot of distances over time for one device."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4aae9718-7fb4-4aa4-b4ba-1509789af1f5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "dm_clean = (spark.table(\"workspace.bronze.device_message_raw\")\n",
    "    .withColumn(\"distance_cm\", F.regexp_replace(F.col(\"distance\").cast(\"string\"), \"[^0-9.]\", \"\").cast(\"double\"))\n",
    "    .withColumn(\"ts_ms\", F.col(\"timestamp\").cast(\"bigint\")))\n",
    "\n",
    "sample_device = dm_clean.select(\"device_id\").first()[\"device_id\"]\n",
    "\n",
    "pdf = (dm_clean\n",
    "       .filter(F.col(\"device_id\") == sample_device)\n",
    "       .orderBy(\"ts_ms\")\n",
    "       .limit(1000)\n",
    "       .select(\"ts_ms\", \"distance_cm\")\n",
    "       .toPandas())\n",
    "\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.plot(pdf[\"ts_ms\"], pdf[\"distance_cm\"], color='blue', marker='o', markersize=2)\n",
    "plt.title(f\"Variação da Distância - Dispositivo: {sample_device}\")\n",
    "plt.xlabel(\"Tempo (ms)\")\n",
    "plt.ylabel(\"Distância (cm)\")\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4cb21360-b81c-4ee9-be7f-84e6286a1003",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Final Reflection\n",
    "* What was easy: Navigating the Catalog UI to upload Parquet files was straightforward, as was performing basic SQL queries to verify the Bronze layer data.\n",
    "\n",
    "* What was confusing: Understanding the difference between a Managed table and an External table in the Databricks workspace took some time. Additionally, reconciling column naming differences (snake_case vs. CamelCase) between the raw data and the lab instructions required careful debugging.\n",
    "\n",
    "* Ethics Risk: The customer and device_id columns could be used to identify specific individuals. If this data were leaked, it would expose their personal physical activity levels and daily routines. If we mislabel or incorrectly clean sensor data, it could lead to an AI model giving a inaccurate physical assessments, which could potentially harm a user's health."
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "stedi_step_tests_python",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
