{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6c563a90-377b-48ba-a663-d9057892004b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Part 2: Python Data Transformation\n",
    "In this section, I am using PySpark to convert the raw numeric timestamps into human-readable Date/Time formats for better analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "46d60c25-5083-4606-97d6-3637b91aa77e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import from_unixtime, col\n",
    "\n",
    "# Load the device messages\n",
    "df_messages = spark.read.table(\"workspace.bronze.device_message_raw\")\n",
    "\n",
    "# Convert the 'timestamp' to a readable timestamp\n",
    "df_refined = df_messages.withColumn(\"readable_time\", from_unixtime(col(\"timestamp\") / 1000))\n",
    "\n",
    "# Display the results\n",
    "display(df_refined.select(\"device_id\", \"timestamp\", \"readable_time\").limit(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e7e92255-f17b-43c2-9889-619eb1894d77",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "###Analyzing Step Test Metrics\n",
    "Loading the step test data to calculate the average steps per customer using Python logic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9250787c-08b6-4566-ad0f-12975f1dd560",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Load the step test data\n",
    "df_steps = spark.read.table(\"workspace.bronze.Rapid_step_test_raw\")\n",
    "\n",
    "# Group by customer and find the average total_steps\n",
    "df_avg_steps = df_steps.groupBy(\"customer\").avg(\"total_steps\")\n",
    "\n",
    "display(df_avg_steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4cb21360-b81c-4ee9-be7f-84e6286a1003",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Final Reflection\n",
    "* What was easy: Navigating the Catalog UI to upload Parquet files was straightforward.\n",
    "\n",
    "* What was confusing: Understanding the difference between a Managed table and an External table in the Databricks workspace took some time.\n",
    "\n",
    "* Ethics Risk: The customer and device_id columns could be used to identify specific individuals. If this data were leaked, it would expose their personal physical activity levels and daily routines."
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "stedi_step_tests_python",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
