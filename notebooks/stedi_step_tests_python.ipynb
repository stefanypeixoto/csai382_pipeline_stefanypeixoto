{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6c563a90-377b-48ba-a663-d9057892004b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Part 2: Python Data Transformation\n",
    "In this section, I am using PySpark to convert the raw numeric timestamps into human-readable Date/Time formats for better analysis.\n",
    "\n",
    "This Python section mirrors the SQL steps, but prepares a compact features table (avg/min/max/variance of distance within each test window). We will reuse this features table in ML weeks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "46d60c25-5083-4606-97d6-3637b91aa77e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "dm = spark.read.table(\"workspace.bronze.device_message_raw\").withColumn(\"source_table\", F.lit(\"device_message_raw\"))\n",
    "rt = spark.read.table(\"workspace.bronze.Rapid_step_test_raw\").withColumn(\"source_table\", F.lit(\"Rapid_step_test_raw\"))\n",
    "\n",
    "dm.printSchema()\n",
    "rt.printSchema()\n",
    "\n",
    "df_refined = dm.withColumn(\"readable_time\", F.from_unixtime(F.col(\"timestamp\") / 1000))\n",
    "display(df_refined.select(\"source_table\", \"device_id\", \"timestamp\", \"readable_time\").limit(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "55caf2dc-e09c-4947-805e-7726b2e15ed2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "###Per-device descriptive stats\n",
    "\n",
    "Showing summarized stats for each device."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5f1f57a7-5c34-40a4-b7e5-28ddadaaf778",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dm_clean = (dm\n",
    "    .withColumn(\"distance_cm\", F.regexp_replace(F.col(\"distance\").cast(\"string\"), \"[^0-9.]\", \"\").cast(\"double\"))\n",
    "    .withColumn(\"ts_ms\", F.col(\"timestamp\").cast(\"bigint\")))\n",
    "\n",
    "dm_stats = (dm_clean.groupBy(\"device_id\")\n",
    "    .agg(F.count(\"*\").alias(\"n\"), F.avg(\"distance_cm\").alias(\"avg_cm\"))\n",
    "    .orderBy(F.desc(\"n\")))\n",
    "display(dm_stats.limit(20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ed1c1d63-6664-4da7-9b68-5f82660d7305",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "###Explode stepPoints to long form\n",
    "\n",
    "This is the python version of the explode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f46b13ff-ea1c-440c-8920-cc0b88291c1f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "rt = spark.table(\"workspace.bronze.rapid_step_test_raw\")\n",
    "rt_exploded = rt.select(\"customer\", \"device_id\", \"start_time\", \"stop_time\", \n",
    "                        F.posexplode(\"step_points\").alias(\"step_index\", \"step_ms\"))\n",
    "display(rt_exploded.limit(20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "02a025c7-b1bb-4979-97b2-18dbc4b93967",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "###Windowed join: feature prep\n",
    "\n",
    "This Python section mirrors the SQL steps, but prepares a compact features table. We will reuse this features table in ML weeks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d42488d9-095c-4fcc-92ed-328e82fddb84",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 1. Prepare messages (Left side)\n",
    "msgs = dm_clean.select(\"source_table\", \"device_id\", \"ts_ms\", \"distance_cm\")\n",
    "\n",
    "# 2. Join and drop the duplicate device_id from the test table (rt)\n",
    "curated_df = msgs.alias(\"m\").join(\n",
    "    rt.alias(\"t\"),\n",
    "    (F.col(\"m.device_id\") == F.col(\"t.device_id\")) & \n",
    "    (F.col(\"m.ts_ms\").between(F.col(\"t.start_time\"), F.col(\"t.stop_time\"))),\n",
    "    how=\"left\"\n",
    ").drop(rt.device_id) # This line prevents the \"Column Already Exists\" error\n",
    "\n",
    "# 3. Add the Label logic\n",
    "curated_labeled = curated_df.withColumn(\n",
    "    \"label\", \n",
    "    F.when(F.col(\"t.start_time\").isNotNull(), \"step\").otherwise(\"no_step\")\n",
    ")\n",
    "\n",
    "# 4. Display to verify\n",
    "display(curated_labeled.select(\"m.source_table\", \"device_id\", \"ts_ms\", \"label\").limit(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1331541b-7774-47e9-97df-3a264746c8b5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Verification: Count the labels\n",
    "display(curated_labeled.groupBy(\"label\").count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a8319d0e-4bcd-45e7-a7bb-aa8548eb7f1a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Save the curated dataset as the Silver Layer\n",
    "curated_labeled.write.mode(\"overwrite\").saveAsTable(\"workspace.silver.curated_stedi_data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c9e023d7-0314-4b24-b36d-99c14a5ecb6d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "###Quick Visual Check\n",
    "\n",
    "Creating a simple line plot of distances over time for one device."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4aae9718-7fb4-4aa4-b4ba-1509789af1f5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "sample_device = dm_clean.select(\"device_id\").first()[\"device_id\"]\n",
    "\n",
    "pdf = (dm_clean\n",
    "       .filter(F.col(\"device_id\") == sample_device)\n",
    "       .orderBy(\"ts_ms\")\n",
    "       .limit(1000)\n",
    "       .select(\"ts_ms\", \"distance_cm\")\n",
    "       .toPandas())\n",
    "\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.plot(pdf[\"ts_ms\"], pdf[\"distance_cm\"], color='blue', marker='o', markersize=2)\n",
    "plt.title(f\"Variação da Distância - Dispositivo: {sample_device}\")\n",
    "plt.xlabel(\"Tempo (ms)\")\n",
    "plt.ylabel(\"Distância (cm)\")\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4cb21360-b81c-4ee9-be7f-84e6286a1003",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Final Reflection\n",
    "* What was easy: Navigating the Catalog UI to upload Parquet files was straightforward, as was performing basic SQL queries to verify the Bronze layer data.\n",
    "\n",
    "* What was confusing: Understanding the difference between a Managed table and an External table in the Databricks workspace took some time. Additionally, reconciling column naming differences (snake_case vs. CamelCase) between the raw data and the lab instructions required careful debugging.\n",
    "\n",
    "* Ethics Risk: The customer and device_id columns could be used to identify specific individuals. If this data were leaked, it would expose their personal physical activity levels and daily routines. If we mislabel or incorrectly clean sensor data, it could lead to an AI model giving a inaccurate physical assessments, which could potentially harm a user's health."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ad2ee725-cb05-4423-9a44-8d1cee142c93",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "###Ethics Check\n",
    "\n",
    "- Are we labeling data fairly? Yes. By using a left join, we include both step and no_step data. This prevents the model from being biased and ensures it understands what \"standing still\" looks like.\n",
    "\n",
    "- Are we protecting identity? We are using device_id and customer (UUIDs) instead of names or PII (Personally Identifiable Information). However, we must ensure these IDs cannot be linked back to a master customer list outside this environment.\n",
    "\n",
    "- Are we avoiding medical claims? Yes. We are strictly labeling physical movement (mechanical steps), not diagnosing health conditions or making medical \"gait\" assessments."
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "stedi_step_tests_python",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
