{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cedda7c9-88b3-4c10-9f30-6c34e9291e03",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "# 1. Load\n",
    "dm = spark.read.table(\"workspace.bronze.device_message_raw\").withColumn(\"source_label\", F.lit(\"device\"))\n",
    "rt = spark.read.table(\"workspace.bronze.Rapid_step_test_raw\").withColumn(\"source_label\", F.lit(\"step\"))\n",
    "\n",
    "# 2. Clean\n",
    "dm_clean = dm.withColumn(\"distance_cm\", F.regexp_replace(F.col(\"distance\").cast(\"string\"), \"[^0-9.]\", \"\").cast(\"double\")) \\\n",
    "             .withColumn(\"ts_ms\", F.col(\"timestamp\").cast(\"bigint\"))\n",
    "\n",
    "# 3. Join & Label (Dropping BOTH duplicate columns)\n",
    "final_df = dm_clean.alias(\"m\").join(\n",
    "    rt.alias(\"t\"),\n",
    "    (F.col(\"m.device_id\") == F.col(\"t.device_id\")) & \n",
    "    (F.col(\"m.ts_ms\").between(F.col(\"t.start_time\"), F.col(\"t.stop_time\"))),\n",
    "    how=\"left\"\n",
    ").drop(rt.device_id).drop(rt.source_label) # Drop both duplicates here\n",
    "\n",
    "final_df = final_df.withColumn(\"step_label\", \n",
    "    F.when(F.col(\"t.start_time\").isNotNull(), \"step\").otherwise(\"no_step\")\n",
    ")\n",
    "\n",
    "# 4. Save to the final table name required by the validation query\n",
    "final_df.write.mode(\"overwrite\").saveAsTable(\"labeled_step_test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c3f2773e-a4e5-4fe1-9e23-61003d7c8ee3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- Steps vs. No-Steps\n",
    "SELECT step_label, COUNT(*)\n",
    "FROM labeled_step_test\n",
    "GROUP BY step_label;\n",
    "\n",
    "-- Invalid or missing labels\n",
    "SELECT *\n",
    "FROM labeled_step_test\n",
    "WHERE step_label NOT IN ('step', 'no_step')\n",
    "  OR step_label IS NULL\n",
    "\n",
    "\n",
    "LIMIT 50;\n",
    "\n",
    "-- Source label counts\n",
    "SELECT source_label, COUNT(*)\n",
    "FROM labeled_step_test\n",
    "GROUP BY source_label;\n",
    "\n",
    "-- Invalid source labels\n",
    "SELECT *\n",
    "FROM labeled_step_test\n",
    "WHERE source_label NOT IN ('device', 'step')\n",
    "   OR source_label IS NULL\n",
    "LIMIT 50;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a43170ee-2142-477e-b7a1-215e9293ca91",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##The Ethics Reflection\n",
    "\n",
    "Automating health data pipelines introduces a high responsibility for accuracy, as errors in the ETL process can propagate silently without human intervention. Engineers must implement rigorous validation checks to ensure that automated labels like 'step' or 'no_step' remain accurate over time, preventing biased datasets that could lead to incorrect health insights. We also have a duty to protect individual privacy by ensuring that automated workflows do not inadvertently expose PII or make diagnostic claims that exceed the data's intent. Ultimately, automation should serve to protect the integrity of the person's health narrative, not just the efficiency of the data flow."
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 7100416263688201,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "03_4_stedi_etl_pipeline",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
